{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1I05_5ytfmjzfECk-LnO-ufNq4OUc9NkG","timestamp":1752503483429},{"file_id":"1OPC2lMhzAqeZqqSzzkvG8scilMzxt65z","timestamp":1743173636550}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn  # Import neural network module\n","import torch.optim as optim  # Import optimization module\n","from torchvision import datasets, transforms  # Import for datasets and transformations"],"metadata":{"id":"sqh6_lYAQTso"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGBE3fvPEpQK"},"outputs":[],"source":["# Data preparation\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert images to PyTorch tensors\n","    # transforms.Normalize((0.5,), (0.5,))  # Normalize with mean 0.5 and standard deviation 0.5\n","])"]},{"cell_type":"code","source":["# Load MNIST datasets, applying the defined transformations\n","train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n","\n","# Create DataLoaders for efficient training and testing data handling\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JalNcZxvQiKZ","executionInfo":{"status":"ok","timestamp":1743173927497,"user_tz":-540,"elapsed":1791,"user":{"displayName":"JongHyun Kim","userId":"04750869897114690179"}},"outputId":"0daf4c4f-eccc-4ca7-aad8-3975cf57c6ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 54.1MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 14.3MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 4.40MB/s]\n"]}]},{"cell_type":"code","source":["# Model definition\n","model = nn.Sequential(\n","    nn.Flatten(),     # Flatten images into a single vector\n","    nn.Linear(784, 128),  # Fully connected layer with 128 neurons\n","    nn.ReLU(),           # ReLU activation for non-linearity\n","    nn.Linear(128, 10)   # Output layer with 10 neurons (for 10 classes in MNIST)\n",")"],"metadata":{"id":"OntMaVSYQYmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss function and optimizer\n","loss_fn = nn.CrossEntropyLoss()  # Common loss function for classification\n","optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent optimizer"],"metadata":{"id":"jgwblLFQQb7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(test_loader.dataset), len(test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6urnClhRcg6","executionInfo":{"status":"ok","timestamp":1743174167334,"user_tz":-540,"elapsed":51,"user":{"displayName":"JongHyun Kim","userId":"04750869897114690179"}},"outputId":"40faa73a-6abc-4e2e-b1b0-2b47e9bb7c72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 157)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Training and Evaluation loop\n","for epoch in range(5):  # Loop for 5 epochs\n","    model.train()  # Set the model to training mode\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Iterate over batches of data\n","        optimizer.zero_grad()  # Clear gradients from the previous iteration\n","        output = model(data)  # Forward pass through the model\n","        loss = loss_fn(output, target)  # Calculate the loss\n","        loss.backward()  # Compute gradients (backpropagation)\n","        optimizer.step()  # Update model parameters\n","\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Disable gradient calculations for efficiency\n","        for data, target in test_loader:  # Iterate over test data\n","            output = model(data)\n","            test_loss += loss_fn(output, target).item()  # Accumulate test loss\n","            pred = output.argmax(dim=1, keepdim=True)  # Get predicted class\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Update correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate average test loss\n","\n","    print('Epoch: {}, Test Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        epoch, test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GbQb9kPEQONC","executionInfo":{"status":"ok","timestamp":1743174043266,"user_tz":-540,"elapsed":98053,"user":{"displayName":"JongHyun Kim","userId":"04750869897114690179"}},"outputId":"a9775d10-7f5a-421f-e010-8c98953b4fd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Test Loss: 0.0060, Accuracy: 8962/10000 (90%)\n","Epoch: 1, Test Loss: 0.0051, Accuracy: 9060/10000 (91%)\n","Epoch: 2, Test Loss: 0.0048, Accuracy: 9128/10000 (91%)\n","Epoch: 3, Test Loss: 0.0043, Accuracy: 9220/10000 (92%)\n","Epoch: 4, Test Loss: 0.0040, Accuracy: 9236/10000 (92%)\n"]}]}]}