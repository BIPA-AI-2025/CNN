import streamlit as st
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.transforms import functional as TF
from PIL import Image

# ------------------------------------------
# ğŸ”§ ëª¨ë¸ ë¸”ë¡ ì •ì˜
# ------------------------------------------
class MyConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch, dropout_p):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(),
            nn.Dropout(dropout_p),
            nn.MaxPool2d(2, stride=2)
        )

    def forward(self, x):
        return self.model(x)

# ------------------------------------------
# ğŸ”§ ASL ëª¨ë¸ ì•„í‚¤í…ì²˜
# ------------------------------------------
IMG_CHS = 1
IMG_WIDTH, IMG_HEIGHT = 28, 28
N_CLASSES = 24

class ASLModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            MyConvBlock(IMG_CHS, 25, 0.0),
            MyConvBlock(25, 50, 0.2),
            MyConvBlock(50, 75, 0.0),
            nn.Flatten(),
            nn.Linear(75 * 3 * 3, 512),
            nn.Dropout(0.3),
            nn.ReLU(),
            nn.Linear(512, N_CLASSES)
        )

    def forward(self, x):
        return self.net(x)

# ------------------------------------------
# ğŸ”  ë¼ë²¨ ë§¤í•‘ (J, Z ì œì™¸)
# ------------------------------------------
label_map = {
    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E',
    5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'K',
    10: 'L', 11: 'M', 12: 'N', 13: 'O', 14: 'P',
    15: 'Q', 16: 'R', 17: 'S', 18: 'T', 19: 'U',
    20: 'V', 21: 'W', 22: 'X', 23: 'Y'
}

# ------------------------------------------
# âœ… ëª¨ë¸ ë¡œë“œ
# ------------------------------------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_path = './asl_cnn_model.pth'
model = torch.load(model_path, map_location=device)
model.to(device)
model.eval()

# ------------------------------------------
# ğŸ”§ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ------------------------------------------
def preprocess_image(image_pil):
    # ğŸ“ Step 1: ì¤‘ì•™ ì •ì‚¬ê°í˜• crop
    width, height = image_pil.size
    min_dim = min(width, height)
    left = (width - min_dim) // 2
    top = (height - min_dim) // 2
    image_cropped = image_pil.crop((left, top, left + min_dim, top + min_dim))

    # Step 2: ì „ì²˜ë¦¬ (Grayscale + Resize + ToTensor)
    transform = transforms.Compose([
        transforms.Grayscale(),                    
        transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),
        transforms.ToTensor()
    ])
    return transform(image_cropped)

# ------------------------------------------
# ğŸ–¼ï¸ Streamlit UI
# ------------------------------------------
st.title("ğŸ¤Ÿ ASL ì•ŒíŒŒë²³ ì˜ˆì¸¡ê¸°")
st.write("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•˜ì—¬ ì† ëª¨ì–‘ì„ ì˜ˆì¸¡í•´ë³´ì„¸ìš”.")

tab1, tab2 = st.tabs(["ğŸ“ ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ğŸ“¸ ì¹´ë©”ë¼ ì´¬ì˜"])

# ì´ë¯¸ì§€ ì—…ë¡œë“œ
with tab1:
    uploaded_file = st.file_uploader("ASL ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['png', 'jpg', 'jpeg'])
    if uploaded_file:
        uploaded_image = Image.open(uploaded_file).convert("RGB")
        st.image(uploaded_image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)

# ì¹´ë©”ë¼ ì…ë ¥
with tab2:
    camera_image = st.camera_input("ì¹´ë©”ë¼ë¡œ ì† ëª¨ì–‘ì„ ì´¬ì˜í•˜ì„¸ìš”")
    if camera_image:
        camera_image_pil = Image.open(camera_image).convert("RGB")
        st.image(camera_image_pil, caption="ì¹´ë©”ë¼ ì´ë¯¸ì§€", use_container_width=True)
    
# ì˜ˆì¸¡ ë²„íŠ¼
if st.button("Predict"):
    if uploaded_file:
        input_image = uploaded_image
    elif camera_image:
        input_image = camera_image_pil
    else:
        st.warning("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
    input_tensor = preprocess_image(input_image).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(input_tensor)
        probs = torch.softmax(outputs, dim=1)
        confidence, pred_idx = torch.max(probs, dim=1)
        pred_label = label_map[int(pred_idx)]

    st.success(f"ì˜ˆì¸¡ ê²°ê³¼: **{pred_label}** ({float(confidence)*100:.2f}%)")
