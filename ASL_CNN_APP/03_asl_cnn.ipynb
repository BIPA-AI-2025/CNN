{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weabkZTF3ZZM"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz8YI6Fb3ZZN"
   },
   "source": [
    "# 3. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UWR4l4X3ZZN"
   },
   "source": [
    "In the previous section, we built and trained a simple model to classify ASL images. The model was able to learn how to correctly classify the training dataset with very high accuracy, but, it did not perform nearly as well on validation dataset. This behavior of not generalizing well to non-training data is called [overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html), and in this section, we will introduce a popular kind of model called a [convolutional neural network](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) that is especially good for reading images and classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmRLS07k3ZZN"
   },
   "source": [
    "## 3.1 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iuvwj_tr3ZZN"
   },
   "source": [
    "* Prep data specifically for a CNN\n",
    "* Create a more sophisticated CNN model, understanding a greater variety of model layers\n",
    "* Train a CNN model and observe its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4999,
     "status": "ok",
     "timestamp": 1732281477579,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "9kMRTHEV2AFm",
    "outputId": "04229d88-37aa-42ac-f1bd-ccfd557182ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37687,
     "status": "ok",
     "timestamp": 1732281539766,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "iUEFVgCWBfHW",
    "outputId": "c04ae1d1-e14e-4b7f-d8d4-b47a515649f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEGukATl3ZZN"
   },
   "source": [
    "## 3.2 Loading and Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfsmjD5yBMWE"
   },
   "source": [
    "### 3.2.1 Preparing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SyD7hID3ZZN"
   },
   "source": [
    "Let's begin by loading our DataFrames like we did in the previous lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMMgEMcc2Ehg"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/content/drive/MyDrive/강의자료/DLI_FDL_EN/LAB/data/asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"/content/drive/MyDrive/강의자료/DLI_FDL_EN/LAB/data/asl_data/sign_mnist_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1732282112051,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "FkJov9y9BMWF",
    "outputId": "6a6a6f67-e4ac-46d3-9649-c7d25060b430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 785), (7172, 785))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHvEvdutBMWG"
   },
   "source": [
    "This ASL data is already flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1732282194201,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "SPY0m3dcBMWG",
    "outputId": "6bab3129-b971-45b2-ff0f-33367799ebca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107, 118, 127, ..., 204, 203, 202],\n",
       "       [155, 157, 156, ..., 103, 135, 149],\n",
       "       [187, 188, 188, ..., 195, 194, 195],\n",
       "       [211, 211, 212, ..., 222, 229, 163],\n",
       "       [164, 167, 170, ..., 163, 164, 179]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = train_df.head().copy()  # Grab the top 5 rows\n",
    "sample_y = sample_df.pop('label')\n",
    "sample_x = sample_df.values\n",
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1732282197095,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "jkY3m3HpBMWG",
    "outputId": "82d9188e-2b19-4eab-92ab-4f3b8af7d86e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 784),\n",
       " (5,),\n",
       " 0     3\n",
       " 1     6\n",
       " 2     2\n",
       " 3     2\n",
       " 4    12\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape, sample_y.shape, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1732282215035,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "HaYvGyN8BMWG",
    "outputId": "c6ccc05d-8289-4b71-b8c0-8e1ea673b803"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "sample_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-aa48c5d4-4add-49be-ac53-69069b36a463\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>157</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa48c5d4-4add-49be-ac53-69069b36a463')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-aa48c5d4-4add-49be-ac53-69069b36a463 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-aa48c5d4-4add-49be-ac53-69069b36a463');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-e8a58032-4fb0-459f-ad8d-ff60123dd6ec\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8a58032-4fb0-459f-ad8d-ff60123dd6ec')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-e8a58032-4fb0-459f-ad8d-ff60123dd6ec button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_4283b6b9-9139-4b94-9482-971677fb779a\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sample_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_4283b6b9-9139-4b94-9482-971677fb779a button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('sample_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0     107     118     127     134     139     143     146     150     153   \n",
       "1     155     157     156     156     156     157     156     158     158   \n",
       "2     187     188     188     187     187     186     187     188     187   \n",
       "3     211     211     212     212     211     210     211     210     210   \n",
       "4     164     167     170     172     176     179     180     184     185   \n",
       "\n",
       "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0      156  ...       207       207       207       207       206       206   \n",
       "1      157  ...        69       149       128        87        94       163   \n",
       "2      186  ...       202       201       200       199       198       199   \n",
       "3      211  ...       235       234       233       231       230       226   \n",
       "4      186  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY935RqFBMWH"
   },
   "source": [
    "In this format, we don't have all the information about which pixels are near each other. Because of this, we can't apply convolutions that will detect features. Let's [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) our dataset so that they are in a 28x28 pixel format. This will allow our convolutions to associate groups of pixels and detect important features.\n",
    "\n",
    "Note that for the first convolutional layer of our model, we need to have not only the height and width of the image, but also the number of [color channels](https://www.photoshopessentials.com/essentials/rgb/). Our images are grayscale, so we'll just have 1 channel.\n",
    "\n",
    "That means that we need to convert the current shape `(5, 784)` to `(5, 1, 28, 28)`. With [NumPy](https://numpy.org/doc/stable/index.html) arrays, we can pass a `-1` for any dimension we wish to remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1732282289415,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "_xUYW4rtBMWH",
    "outputId": "369fa827-3f31-4860-bbe1-d406ff3ea7fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "IMG_CHS = 1\n",
    "\n",
    "sample_x = sample_x.reshape(-1, IMG_CHS, IMG_HEIGHT, IMG_WIDTH)\n",
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7tKbiTBBMWH"
   },
   "source": [
    "### 3.2.2 Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o280Trc-BMWH"
   },
   "source": [
    "Let's add the steps above into our `MyDataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH9it611BMWH"
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSnhV7hzBMWH"
   },
   "source": [
    "There are 4 `FIXME`s in the class definition below. Can you replace them with the correct values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpzGOri32Klj"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()  # Some operations below are in-place\n",
    "        y_df = x_df.pop('label')\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, 1, 28, 28)\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuABSmiRBMWI"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AEP_3C8BMWI"
   },
   "source": [
    "Click the `...` below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh74nACZBMWI"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()  # Some operations below are in-place\n",
    "        y_df = x_df.pop('label')\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo90E9OJBMWI"
   },
   "source": [
    "### 3.2.3 Create a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBnsNLG2BMWI"
   },
   "source": [
    "Next, let's create the DataLoader from the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS7SHGtrBMWI"
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk63QG_ZBMWI"
   },
   "source": [
    "One of these function calls is missing the `shuffle=True` argument. Can you remember which one it is and add it back in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1732282857484,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "unf8Cz4WcK_M",
    "outputId": "4e981f8c-576a-42e8-f018-ba9d09d4c25f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 7172)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = MyDataset(train_df)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_data = MyDataset(valid_df)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "valid_N = len(valid_loader.dataset)\n",
    "\n",
    "train_N, valid_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf4QEPTPBMWJ",
    "outputId": "744290c8-2852-4a29-b134-0611106bf916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[0.4196, 0.4627, 0.4980, 0.5255, 0.5451, 0.5608, 0.5725, 0.5882,\n",
       "            0.6000, 0.6118, 0.6196, 0.6275, 0.6392, 0.6471, 0.6235, 0.6510,\n",
       "            0.6588, 0.6667, 0.6667, 0.6706, 0.6706, 0.6706, 0.6745, 0.6706,\n",
       "            0.6706, 0.6667, 0.6667, 0.6627],\n",
       "           [0.4353, 0.4745, 0.5059, 0.5294, 0.5529, 0.5647, 0.5804, 0.5922,\n",
       "            0.6039, 0.6157, 0.6275, 0.6392, 0.6431, 0.6667, 0.4667, 0.5961,\n",
       "            0.6706, 0.6706, 0.6667, 0.6706, 0.6745, 0.6745, 0.6745, 0.6745,\n",
       "            0.6745, 0.6706, 0.6706, 0.6667],\n",
       "           [0.4431, 0.4824, 0.5137, 0.5373, 0.5569, 0.5686, 0.5882, 0.5961,\n",
       "            0.6078, 0.6196, 0.6314, 0.6392, 0.6431, 0.6745, 0.4118, 0.5569,\n",
       "            0.6667, 0.6706, 0.6706, 0.6706, 0.6745, 0.6745, 0.6784, 0.6784,\n",
       "            0.6745, 0.6706, 0.6706, 0.6706],\n",
       "           [0.4549, 0.4902, 0.5216, 0.5451, 0.5608, 0.5725, 0.5922, 0.6000,\n",
       "            0.6118, 0.6235, 0.6353, 0.6392, 0.6549, 0.6549, 0.3725, 0.5647,\n",
       "            0.6706, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6784, 0.6784,\n",
       "            0.6784, 0.6745, 0.6745, 0.6706],\n",
       "           [0.4588, 0.4941, 0.5255, 0.5490, 0.5686, 0.5843, 0.6000, 0.6118,\n",
       "            0.6196, 0.6314, 0.6392, 0.6431, 0.6863, 0.6118, 0.3412, 0.6039,\n",
       "            0.6745, 0.6784, 0.6784, 0.6784, 0.6784, 0.6784, 0.6824, 0.6824,\n",
       "            0.6824, 0.6784, 0.6745, 0.6745],\n",
       "           [0.4667, 0.5020, 0.5333, 0.5569, 0.5725, 0.5882, 0.6000, 0.6118,\n",
       "            0.6235, 0.6392, 0.6471, 0.6431, 0.7216, 0.5804, 0.3490, 0.6431,\n",
       "            0.6745, 0.6824, 0.6824, 0.6824, 0.6824, 0.6863, 0.6863, 0.6824,\n",
       "            0.6863, 0.6824, 0.6784, 0.6784],\n",
       "           [0.4784, 0.5098, 0.5412, 0.5608, 0.5765, 0.5882, 0.6039, 0.6196,\n",
       "            0.6353, 0.6471, 0.6510, 0.6745, 0.7098, 0.5020, 0.3686, 0.6667,\n",
       "            0.6784, 0.6863, 0.6824, 0.6863, 0.6902, 0.6941, 0.6941, 0.6941,\n",
       "            0.6941, 0.6863, 0.6863, 0.6824],\n",
       "           [0.4784, 0.5176, 0.5451, 0.5686, 0.5843, 0.5961, 0.6118, 0.6275,\n",
       "            0.6392, 0.6471, 0.6510, 0.7098, 0.6745, 0.4039, 0.4431, 0.6863,\n",
       "            0.6902, 0.6980, 0.6980, 0.7020, 0.7020, 0.7020, 0.7020, 0.6980,\n",
       "            0.7020, 0.6941, 0.6863, 0.6824],\n",
       "           [0.4902, 0.5255, 0.5529, 0.5765, 0.5882, 0.6000, 0.6157, 0.6314,\n",
       "            0.6431, 0.6549, 0.6588, 0.7216, 0.7020, 0.4549, 0.4941, 0.6471,\n",
       "            0.6902, 0.7020, 0.7059, 0.7059, 0.7098, 0.7059, 0.7059, 0.7059,\n",
       "            0.7020, 0.6980, 0.6941, 0.6902],\n",
       "           [0.5020, 0.5294, 0.5569, 0.5804, 0.5961, 0.6039, 0.6196, 0.6353,\n",
       "            0.6471, 0.6588, 0.6667, 0.7333, 0.7059, 0.6118, 0.6314, 0.4863,\n",
       "            0.5608, 0.7020, 0.6980, 0.6980, 0.7098, 0.7137, 0.7098, 0.7059,\n",
       "            0.7098, 0.7059, 0.7020, 0.7020],\n",
       "           [0.5059, 0.5333, 0.5647, 0.5882, 0.6000, 0.6078, 0.6235, 0.6392,\n",
       "            0.6510, 0.6627, 0.6745, 0.7333, 0.7216, 0.6000, 0.4000, 0.4588,\n",
       "            0.4314, 0.6863, 0.6627, 0.6039, 0.7137, 0.7176, 0.7176, 0.7137,\n",
       "            0.7137, 0.7098, 0.7098, 0.7020],\n",
       "           [0.5137, 0.5412, 0.5686, 0.5882, 0.6078, 0.6157, 0.6314, 0.6471,\n",
       "            0.6588, 0.6824, 0.7451, 0.7412, 0.6863, 0.5725, 0.3686, 0.3804,\n",
       "            0.4431, 0.5922, 0.6196, 0.5059, 0.7216, 0.7216, 0.7216, 0.7216,\n",
       "            0.7176, 0.7176, 0.7137, 0.7059],\n",
       "           [0.5137, 0.5451, 0.5725, 0.5922, 0.6078, 0.6235, 0.6392, 0.6549,\n",
       "            0.6863, 0.7137, 0.7020, 0.6706, 0.6235, 0.4471, 0.4000, 0.3490,\n",
       "            0.4745, 0.5333, 0.5333, 0.3765, 0.6745, 0.7294, 0.7294, 0.7255,\n",
       "            0.7255, 0.7216, 0.7137, 0.7098],\n",
       "           [0.5137, 0.5490, 0.5765, 0.6039, 0.6157, 0.6275, 0.6431, 0.7020,\n",
       "            0.7294, 0.7490, 0.7333, 0.7059, 0.6157, 0.3922, 0.3451, 0.3294,\n",
       "            0.4235, 0.4353, 0.4941, 0.3529, 0.4706, 0.7294, 0.7333, 0.7333,\n",
       "            0.7294, 0.7255, 0.7216, 0.7137],\n",
       "           [0.5216, 0.5529, 0.5843, 0.6078, 0.6196, 0.6275, 0.6824, 0.7882,\n",
       "            0.7412, 0.6471, 0.5922, 0.5608, 0.5725, 0.4706, 0.3412, 0.3059,\n",
       "            0.3412, 0.2980, 0.4235, 0.3843, 0.3765, 0.7098, 0.7373, 0.7333,\n",
       "            0.7294, 0.7294, 0.7255, 0.7176],\n",
       "           [0.5216, 0.5529, 0.5882, 0.6118, 0.6275, 0.6314, 0.7020, 0.7725,\n",
       "            0.6824, 0.5294, 0.3882, 0.2824, 0.3725, 0.5255, 0.3804, 0.2824,\n",
       "            0.2902, 0.2667, 0.4549, 0.4118, 0.4235, 0.7333, 0.7412, 0.7333,\n",
       "            0.7333, 0.7294, 0.7294, 0.7255],\n",
       "           [0.5255, 0.5608, 0.5922, 0.6118, 0.6314, 0.6392, 0.7020, 0.7608,\n",
       "            0.6118, 0.4314, 0.2902, 0.1647, 0.2039, 0.5451, 0.3686, 0.2627,\n",
       "            0.2941, 0.2941, 0.4627, 0.4157, 0.5059, 0.7412, 0.7490, 0.7451,\n",
       "            0.7373, 0.7373, 0.7333, 0.7294],\n",
       "           [0.5294, 0.5647, 0.5961, 0.6196, 0.6392, 0.6392, 0.6941, 0.7569,\n",
       "            0.6314, 0.4784, 0.3294, 0.1686, 0.2784, 0.5255, 0.3176, 0.2235,\n",
       "            0.2784, 0.3451, 0.4392, 0.3843, 0.6157, 0.7569, 0.7569, 0.7529,\n",
       "            0.7451, 0.7451, 0.7412, 0.7373],\n",
       "           [0.5333, 0.5647, 0.5961, 0.6196, 0.6353, 0.6392, 0.6902, 0.7529,\n",
       "            0.6431, 0.5020, 0.3843, 0.2431, 0.2353, 0.3922, 0.2784, 0.2980,\n",
       "            0.3765, 0.3961, 0.4118, 0.3725, 0.6824, 0.7647, 0.7608, 0.7608,\n",
       "            0.7608, 0.7569, 0.7490, 0.7451],\n",
       "           [0.5373, 0.5686, 0.5961, 0.6235, 0.6431, 0.6471, 0.6980, 0.7490,\n",
       "            0.6431, 0.5294, 0.4431, 0.3216, 0.2314, 0.3412, 0.3843, 0.4353,\n",
       "            0.4706, 0.4235, 0.3804, 0.4235, 0.7451, 0.7686, 0.7647, 0.7647,\n",
       "            0.7608, 0.7569, 0.7569, 0.7529],\n",
       "           [0.5451, 0.5725, 0.6039, 0.6275, 0.6431, 0.6471, 0.6863, 0.7294,\n",
       "            0.6392, 0.5451, 0.4392, 0.3333, 0.2627, 0.4000, 0.4941, 0.5216,\n",
       "            0.4941, 0.4118, 0.4078, 0.6902, 0.7725, 0.7765, 0.7725, 0.7686,\n",
       "            0.7647, 0.7647, 0.7608, 0.7569],\n",
       "           [0.5412, 0.5765, 0.6078, 0.6314, 0.6471, 0.6549, 0.6745, 0.7294,\n",
       "            0.6392, 0.5373, 0.4196, 0.3412, 0.2980, 0.4157, 0.4784, 0.4902,\n",
       "            0.4588, 0.3765, 0.6118, 0.7804, 0.7804, 0.7843, 0.7765, 0.7686,\n",
       "            0.7686, 0.7647, 0.7647, 0.7608],\n",
       "           [0.5451, 0.5804, 0.6118, 0.6392, 0.6510, 0.6588, 0.6745, 0.7059,\n",
       "            0.6196, 0.5137, 0.4235, 0.3882, 0.3373, 0.4235, 0.4627, 0.4549,\n",
       "            0.4039, 0.4196, 0.7490, 0.7922, 0.7882, 0.7843, 0.7843, 0.7843,\n",
       "            0.7804, 0.7725, 0.7765, 0.7686],\n",
       "           [0.5490, 0.5843, 0.6157, 0.6431, 0.6588, 0.6549, 0.6941, 0.6980,\n",
       "            0.6078, 0.5137, 0.4627, 0.4118, 0.3412, 0.3922, 0.4157, 0.3922,\n",
       "            0.3765, 0.6431, 0.7922, 0.7922, 0.7922, 0.7922, 0.7922, 0.7882,\n",
       "            0.7843, 0.7804, 0.7804, 0.7765],\n",
       "           [0.5490, 0.5882, 0.6157, 0.6471, 0.6549, 0.6667, 0.7098, 0.6863,\n",
       "            0.5961, 0.5098, 0.4510, 0.3843, 0.3216, 0.3333, 0.3529, 0.3882,\n",
       "            0.6471, 0.7922, 0.7961, 0.8000, 0.7961, 0.7961, 0.7922, 0.7922,\n",
       "            0.7882, 0.7882, 0.7843, 0.7843],\n",
       "           [0.5569, 0.5882, 0.6235, 0.6471, 0.6667, 0.7490, 0.6784, 0.6157,\n",
       "            0.5647, 0.4667, 0.3804, 0.3294, 0.3098, 0.3098, 0.3569, 0.6745,\n",
       "            0.7922, 0.7961, 0.7961, 0.8039, 0.8000, 0.8000, 0.8000, 0.7961,\n",
       "            0.7922, 0.7922, 0.7882, 0.7843],\n",
       "           [0.5569, 0.5922, 0.6275, 0.6471, 0.7373, 0.7451, 0.7333, 0.5882,\n",
       "            0.4667, 0.4275, 0.3333, 0.3098, 0.3098, 0.3059, 0.5373, 0.7961,\n",
       "            0.8039, 0.8078, 0.8078, 0.8118, 0.8118, 0.8078, 0.8078, 0.8000,\n",
       "            0.8039, 0.8000, 0.7961, 0.7922],\n",
       "           [0.5569, 0.5922, 0.6275, 0.6745, 0.7686, 0.7373, 0.7373, 0.7451,\n",
       "            0.5294, 0.3765, 0.3373, 0.3020, 0.3020, 0.3098, 0.6902, 0.8039,\n",
       "            0.8118, 0.8118, 0.8118, 0.8118, 0.8118, 0.8118, 0.8078, 0.8078,\n",
       "            0.8078, 0.8000, 0.7961, 0.7922]]], device='cuda:0'),\n",
       "  tensor(3, device='cuda:0')),\n",
       " (tensor([[[0.5843, 0.5843, 0.5882, 0.5882, 0.5882, 0.5922, 0.5922, 0.5882,\n",
       "            0.5922, 0.5961, 0.5961, 0.5961, 0.5961, 0.5961, 0.6000, 0.6000,\n",
       "            0.5922, 0.5961, 0.5961, 0.6000, 0.5961, 0.5961, 0.5922, 0.5922,\n",
       "            0.5882, 0.5882, 0.5882, 0.5843],\n",
       "           [0.5882, 0.5882, 0.5882, 0.5961, 0.5961, 0.5922, 0.5961, 0.5961,\n",
       "            0.5961, 0.5961, 0.5961, 0.6000, 0.6039, 0.6000, 0.6039, 0.6039,\n",
       "            0.6000, 0.6039, 0.6000, 0.6039, 0.6000, 0.6000, 0.5961, 0.5961,\n",
       "            0.5961, 0.5922, 0.5882, 0.5922],\n",
       "           [0.5882, 0.5922, 0.5922, 0.5961, 0.5961, 0.5961, 0.6000, 0.6000,\n",
       "            0.5961, 0.5961, 0.5961, 0.6000, 0.6039, 0.6039, 0.6078, 0.6078,\n",
       "            0.6039, 0.6039, 0.6078, 0.6078, 0.6078, 0.6078, 0.6039, 0.6000,\n",
       "            0.6000, 0.5922, 0.5922, 0.5961],\n",
       "           [0.5882, 0.5922, 0.5922, 0.5961, 0.5961, 0.5961, 0.6039, 0.6039,\n",
       "            0.6039, 0.6039, 0.6039, 0.6000, 0.6039, 0.6078, 0.6118, 0.6157,\n",
       "            0.6157, 0.6118, 0.6078, 0.6118, 0.6078, 0.6039, 0.6039, 0.6078,\n",
       "            0.5961, 0.6039, 0.6000, 0.6000],\n",
       "           [0.5922, 0.5961, 0.5961, 0.5961, 0.6039, 0.6039, 0.6039, 0.6039,\n",
       "            0.6039, 0.6078, 0.6157, 0.6118, 0.6118, 0.6118, 0.6039, 0.5882,\n",
       "            0.5725, 0.5765, 0.5725, 0.5765, 0.5608, 0.5373, 0.4941, 0.4941,\n",
       "            0.5569, 0.5451, 0.5961, 0.6039],\n",
       "           [0.5961, 0.6000, 0.6000, 0.6039, 0.6039, 0.6078, 0.6039, 0.6078,\n",
       "            0.6078, 0.6039, 0.6000, 0.5882, 0.5647, 0.5608, 0.5686, 0.5451,\n",
       "            0.5569, 0.5647, 0.6157, 0.6157, 0.5765, 0.5451, 0.5020, 0.4667,\n",
       "            0.5098, 0.4431, 0.5765, 0.6118],\n",
       "           [0.5922, 0.6000, 0.6000, 0.6078, 0.6078, 0.6118, 0.6078, 0.5961,\n",
       "            0.5686, 0.5451, 0.5529, 0.5529, 0.5529, 0.6000, 0.6000, 0.5608,\n",
       "            0.5294, 0.5373, 0.5451, 0.5216, 0.4745, 0.4196, 0.3961, 0.4078,\n",
       "            0.4314, 0.4980, 0.6157, 0.6118],\n",
       "           [0.5922, 0.5961, 0.6000, 0.6078, 0.6078, 0.6039, 0.5922, 0.5725,\n",
       "            0.5451, 0.5137, 0.5098, 0.5255, 0.5373, 0.5176, 0.4902, 0.4353,\n",
       "            0.3961, 0.3686, 0.3725, 0.4118, 0.4431, 0.4784, 0.5216, 0.5686,\n",
       "            0.6000, 0.6157, 0.6118, 0.6118],\n",
       "           [0.5961, 0.5961, 0.6039, 0.5961, 0.5922, 0.5882, 0.5843, 0.5843,\n",
       "            0.5451, 0.4784, 0.4078, 0.3843, 0.3608, 0.3216, 0.3176, 0.3176,\n",
       "            0.3333, 0.4471, 0.5686, 0.6157, 0.6275, 0.6353, 0.6314, 0.6235,\n",
       "            0.6157, 0.6118, 0.6118, 0.6118],\n",
       "           [0.5922, 0.5922, 0.5882, 0.5725, 0.5686, 0.5765, 0.5804, 0.5765,\n",
       "            0.5686, 0.5176, 0.3804, 0.2784, 0.2431, 0.2588, 0.3451, 0.4549,\n",
       "            0.5686, 0.6353, 0.6275, 0.6235, 0.6157, 0.6078, 0.6118, 0.6157,\n",
       "            0.6157, 0.6118, 0.6078, 0.6078],\n",
       "           [0.5922, 0.5686, 0.5647, 0.5686, 0.5765, 0.5686, 0.5765, 0.5882,\n",
       "            0.5882, 0.4863, 0.3608, 0.2667, 0.2471, 0.2627, 0.3373, 0.6235,\n",
       "            0.6392, 0.6078, 0.6196, 0.6157, 0.6118, 0.6118, 0.6157, 0.6118,\n",
       "            0.6118, 0.6118, 0.6078, 0.6039],\n",
       "           [0.5608, 0.5647, 0.5686, 0.5686, 0.5608, 0.5765, 0.5961, 0.5961,\n",
       "            0.5020, 0.3529, 0.3098, 0.2667, 0.2510, 0.2745, 0.2627, 0.3294,\n",
       "            0.5765, 0.6431, 0.6157, 0.6196, 0.6157, 0.6157, 0.6157, 0.6118,\n",
       "            0.6157, 0.6118, 0.6118, 0.6078],\n",
       "           [0.5686, 0.5725, 0.5608, 0.5686, 0.5686, 0.5882, 0.5843, 0.5843,\n",
       "            0.5451, 0.4627, 0.3333, 0.2431, 0.2431, 0.2941, 0.2863, 0.2431,\n",
       "            0.2627, 0.5490, 0.6431, 0.6157, 0.6196, 0.6196, 0.6196, 0.6196,\n",
       "            0.6157, 0.6157, 0.6118, 0.6118],\n",
       "           [0.5882, 0.5765, 0.5647, 0.5765, 0.5843, 0.5804, 0.5843, 0.6196,\n",
       "            0.6196, 0.5333, 0.3686, 0.2471, 0.2275, 0.2706, 0.3333, 0.3216,\n",
       "            0.2627, 0.2745, 0.6118, 0.6275, 0.6235, 0.6275, 0.6235, 0.6196,\n",
       "            0.6157, 0.6118, 0.6118, 0.6118],\n",
       "           [0.5765, 0.5804, 0.5765, 0.5686, 0.5804, 0.5961, 0.5922, 0.6275,\n",
       "            0.6000, 0.4667, 0.3373, 0.2588, 0.2510, 0.2471, 0.2706, 0.2941,\n",
       "            0.3059, 0.2235, 0.5098, 0.6471, 0.6196, 0.6235, 0.6196, 0.6235,\n",
       "            0.6196, 0.6157, 0.6157, 0.6157],\n",
       "           [0.5843, 0.5804, 0.5725, 0.5686, 0.5765, 0.5843, 0.5725, 0.5922,\n",
       "            0.5647, 0.4314, 0.3059, 0.2549, 0.2588, 0.2588, 0.2275, 0.2314,\n",
       "            0.2510, 0.3098, 0.5882, 0.6471, 0.6353, 0.6353, 0.6353, 0.6353,\n",
       "            0.6314, 0.6314, 0.6196, 0.6118],\n",
       "           [0.5922, 0.5725, 0.5608, 0.5529, 0.5412, 0.5490, 0.5569, 0.5725,\n",
       "            0.5647, 0.4745, 0.3294, 0.2196, 0.2431, 0.2745, 0.2784, 0.2667,\n",
       "            0.2235, 0.4588, 0.5647, 0.5647, 0.5765, 0.5843, 0.5961, 0.5882,\n",
       "            0.5725, 0.5725, 0.6039, 0.6275],\n",
       "           [0.5765, 0.5647, 0.5608, 0.5569, 0.5490, 0.5569, 0.5725, 0.5922,\n",
       "            0.6039, 0.5137, 0.3333, 0.2314, 0.2000, 0.2353, 0.3333, 0.2706,\n",
       "            0.2510, 0.2980, 0.2941, 0.3098, 0.3176, 0.3098, 0.2980, 0.3255,\n",
       "            0.4392, 0.5529, 0.6392, 0.6392],\n",
       "           [0.5647, 0.5804, 0.5765, 0.5686, 0.5686, 0.5804, 0.5882, 0.6078,\n",
       "            0.5922, 0.4667, 0.2902, 0.2431, 0.2471, 0.2157, 0.2431, 0.2824,\n",
       "            0.2863, 0.3020, 0.2902, 0.2863, 0.2667, 0.3451, 0.4431, 0.5412,\n",
       "            0.6353, 0.6353, 0.6588, 0.6588],\n",
       "           [0.5725, 0.5725, 0.5569, 0.5529, 0.5529, 0.5412, 0.5255, 0.5569,\n",
       "            0.4863, 0.3765, 0.2941, 0.2627, 0.2549, 0.2471, 0.2431, 0.3059,\n",
       "            0.3412, 0.2980, 0.3294, 0.3765, 0.4941, 0.6353, 0.6745, 0.6078,\n",
       "            0.5647, 0.5843, 0.5922, 0.6314],\n",
       "           [0.5569, 0.5333, 0.5176, 0.5255, 0.4980, 0.4667, 0.4627, 0.4667,\n",
       "            0.4039, 0.3412, 0.3020, 0.2863, 0.2745, 0.2431, 0.2510, 0.2824,\n",
       "            0.3647, 0.5255, 0.6078, 0.6275, 0.6510, 0.6118, 0.5882, 0.5922,\n",
       "            0.5608, 0.5333, 0.5686, 0.5843],\n",
       "           [0.5098, 0.5176, 0.4980, 0.4706, 0.4471, 0.4314, 0.4275, 0.4118,\n",
       "            0.3569, 0.3020, 0.2902, 0.2941, 0.2902, 0.2549, 0.2863, 0.4431,\n",
       "            0.6510, 0.6941, 0.6667, 0.6314, 0.5961, 0.5529, 0.5255, 0.5333,\n",
       "            0.5490, 0.5216, 0.4980, 0.5098],\n",
       "           [0.4431, 0.4549, 0.4510, 0.4157, 0.3961, 0.3725, 0.3373, 0.3294,\n",
       "            0.3333, 0.3020, 0.3059, 0.2902, 0.2980, 0.4039, 0.5961, 0.7020,\n",
       "            0.6667, 0.6157, 0.6078, 0.5922, 0.5490, 0.5059, 0.4941, 0.4941,\n",
       "            0.5216, 0.5098, 0.4784, 0.4902],\n",
       "           [0.3176, 0.3373, 0.3333, 0.3255, 0.2980, 0.2824, 0.2863, 0.2980,\n",
       "            0.3020, 0.3098, 0.2784, 0.3961, 0.5922, 0.6980, 0.6941, 0.6667,\n",
       "            0.6314, 0.5961, 0.5765, 0.5922, 0.5216, 0.4510, 0.4745, 0.4745,\n",
       "            0.4863, 0.4941, 0.4784, 0.4784],\n",
       "           [0.2392, 0.2392, 0.2627, 0.2706, 0.2745, 0.2941, 0.3059, 0.3059,\n",
       "            0.3176, 0.2667, 0.4431, 0.6471, 0.6824, 0.6627, 0.6353, 0.6157,\n",
       "            0.5843, 0.5804, 0.5804, 0.5804, 0.4941, 0.3922, 0.4431, 0.4588,\n",
       "            0.4431, 0.4784, 0.4627, 0.4510],\n",
       "           [0.2706, 0.2706, 0.3020, 0.3059, 0.2941, 0.2980, 0.3059, 0.3098,\n",
       "            0.2627, 0.4706, 0.6784, 0.6157, 0.6235, 0.5804, 0.6078, 0.5882,\n",
       "            0.5412, 0.5608, 0.5804, 0.5843, 0.4824, 0.3569, 0.3961, 0.4353,\n",
       "            0.4353, 0.4549, 0.4431, 0.4627],\n",
       "           [0.2902, 0.2941, 0.2980, 0.2941, 0.2941, 0.2980, 0.2941, 0.2667,\n",
       "            0.4863, 0.6745, 0.5961, 0.5725, 0.5725, 0.5725, 0.5961, 0.5569,\n",
       "            0.5137, 0.5255, 0.5647, 0.5765, 0.4902, 0.3412, 0.3412, 0.4039,\n",
       "            0.4196, 0.4314, 0.4549, 0.4431],\n",
       "           [0.2941, 0.2902, 0.2902, 0.2902, 0.2980, 0.2902, 0.3216, 0.5255,\n",
       "            0.6588, 0.6078, 0.5725, 0.5373, 0.5686, 0.5725, 0.5843, 0.5294,\n",
       "            0.4863, 0.4902, 0.5412, 0.5804, 0.4980, 0.3490, 0.3216, 0.3765,\n",
       "            0.4157, 0.4392, 0.4706, 0.4196]]], device='cuda:0'),\n",
       "  tensor(6, device='cuda:0')))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0), valid_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jak2DGEgBMWJ",
    "outputId": "97321ec8-5932-469b-ea81-9be60362967a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 7172)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__(), valid_data.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYlZH7DNBMWJ"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NmecaJABMWK"
   },
   "source": [
    "Click the `...` below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7R2k1NtBMWK"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2fibYyaBMWK"
   },
   "source": [
    "Let's grab a batch from the DataLoader to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715240550382,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "Z4xylt03dz1W",
    "outputId": "80447d85-302d-4549-976b-f4c3ac0f0644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[[0.6275, 0.6392, 0.6510,  ..., 0.6078, 0.5961, 0.5882],\n",
       "            [0.6275, 0.6392, 0.6510,  ..., 0.6118, 0.6000, 0.5961],\n",
       "            [0.6353, 0.6392, 0.6510,  ..., 0.6157, 0.6078, 0.5961],\n",
       "            ...,\n",
       "            [0.7059, 0.7176, 0.7294,  ..., 0.7294, 0.7216, 0.7137],\n",
       "            [0.7020, 0.7137, 0.7294,  ..., 0.7333, 0.7176, 0.7098],\n",
       "            [0.6980, 0.7059, 0.7176,  ..., 0.7294, 0.7137, 0.7059]]],\n",
       "  \n",
       "  \n",
       "          [[[0.6157, 0.6118, 0.6157,  ..., 0.6078, 0.6078, 0.6000],\n",
       "            [0.6196, 0.6196, 0.6235,  ..., 0.6196, 0.6196, 0.6039],\n",
       "            [0.6235, 0.6275, 0.6314,  ..., 0.6235, 0.6314, 0.6157],\n",
       "            ...,\n",
       "            [0.6314, 0.6431, 0.6510,  ..., 0.7373, 0.7294, 0.7294],\n",
       "            [0.3255, 0.3412, 0.3490,  ..., 0.6275, 0.6392, 0.6471],\n",
       "            [0.3176, 0.3137, 0.3059,  ..., 0.3176, 0.3255, 0.3294]]],\n",
       "  \n",
       "  \n",
       "          [[[0.1412, 0.1725, 0.2588,  ..., 0.6588, 0.6510, 0.6471],\n",
       "            [0.1451, 0.1843, 0.2706,  ..., 0.6745, 0.6667, 0.6706],\n",
       "            [0.1451, 0.1882, 0.2784,  ..., 0.6902, 0.6863, 0.6863],\n",
       "            ...,\n",
       "            [0.2706, 0.2784, 0.2784,  ..., 0.0902, 0.0000, 0.0000],\n",
       "            [0.2706, 0.2745, 0.2824,  ..., 0.0706, 0.0000, 0.0000],\n",
       "            [0.2745, 0.2784, 0.2784,  ..., 0.0353, 0.0000, 0.0000]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0.6667, 0.6706, 0.6745,  ..., 0.7490, 0.7490, 0.7529],\n",
       "            [0.6706, 0.6745, 0.6784,  ..., 0.7529, 0.7529, 0.7529],\n",
       "            [0.6706, 0.6745, 0.6745,  ..., 0.7529, 0.7529, 0.7529],\n",
       "            ...,\n",
       "            [0.6980, 0.7098, 0.6941,  ..., 0.7922, 0.7961, 0.7961],\n",
       "            [0.7020, 0.6980, 0.7098,  ..., 0.7961, 0.7961, 0.7961],\n",
       "            [0.7059, 0.6941, 0.7333,  ..., 0.7961, 0.7961, 0.7961]]],\n",
       "  \n",
       "  \n",
       "          [[[0.6353, 0.6510, 0.6667,  ..., 0.7529, 0.7490, 0.7451],\n",
       "            [0.6353, 0.6549, 0.6706,  ..., 0.7529, 0.7490, 0.7490],\n",
       "            [0.6392, 0.6627, 0.6784,  ..., 0.7608, 0.7608, 0.7569],\n",
       "            ...,\n",
       "            [0.5059, 0.5098, 0.5098,  ..., 0.4706, 0.4706, 0.4667],\n",
       "            [0.5098, 0.5098, 0.5098,  ..., 0.4706, 0.4706, 0.4667],\n",
       "            [0.5059, 0.5098, 0.5098,  ..., 0.4706, 0.4745, 0.4706]]],\n",
       "  \n",
       "  \n",
       "          [[[0.1686, 0.1765, 0.1843,  ..., 0.6431, 0.6510, 0.6588],\n",
       "            [0.1647, 0.1765, 0.1922,  ..., 0.6471, 0.6549, 0.6627],\n",
       "            [0.1765, 0.1765, 0.1647,  ..., 0.6510, 0.6588, 0.6667],\n",
       "            ...,\n",
       "            [0.3255, 0.3176, 0.3216,  ..., 0.2039, 0.2078, 0.2275],\n",
       "            [0.3216, 0.3176, 0.3216,  ..., 0.3529, 0.2745, 0.2392],\n",
       "            [0.3176, 0.3176, 0.3216,  ..., 0.3529, 0.2275, 0.2314]]]],\n",
       "         device='cuda:0'),\n",
       "  tensor([ 3, 22, 20,  5,  5,  7, 12, 19, 21, 18, 21, 19, 10, 12, 22, 23,  0, 15,\n",
       "           0, 10, 16, 13, 15,  5, 16, 11, 17, 11,  8,  4, 11, 14],\n",
       "         device='cuda:0')],\n",
       " 32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch, len(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JjyOV05BMWK"
   },
   "source": [
    "It looks different, but let's check the `shape`s to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1715240552534,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "vannMV7sd6R_",
    "outputId": "627858a2-a4ed-467c-cf82-2b7c1a01c13f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1715240553488,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "YHJgP3A7d9lu",
    "outputId": "4a40ceb8-039b-4517-de8a-bdcb814c4164"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6biSPXKJ3ZZP"
   },
   "source": [
    "## 3.3 Creating a Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1732283222360,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "p_bvGpMId_6q",
    "outputId": "a42262b6-0618-4021-8a97-84973880a130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(25, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.2, inplace=False)\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (10): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU()\n",
       "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (13): Flatten(start_dim=1, end_dim=-1)\n",
       "  (14): Linear(in_features=675, out_features=512, bias=True)\n",
       "  (15): Dropout(p=0.3, inplace=False)\n",
       "  (16): ReLU()\n",
       "  (17): Linear(in_features=512, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMG_HEIGHT = 28\n",
    "# IMG_WIDTH = 28\n",
    "# IMG_CHS = 1\n",
    "\n",
    "n_classes = 24\n",
    "kernel_size = 3\n",
    "flattened_img_size = 75 * 3 * 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # First convolution\n",
    "    nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1),  # 25 x 28 x 28\n",
    "    nn.BatchNorm2d(25),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 25 x 14 x 14\n",
    "\n",
    "    # Second convolution\n",
    "    nn.Conv2d(25, 50, kernel_size, stride=1, padding=1),  # 50 x 14 x 14\n",
    "    nn.BatchNorm2d(50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.2),\n",
    "    nn.MaxPool2d(2, stride=2),  # 50 x 7 x 7\n",
    "\n",
    "    # Third convolution\n",
    "    nn.Conv2d(50, 75, kernel_size, stride=1, padding=1),  # 75 x 7 x 7\n",
    "    nn.BatchNorm2d(75),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 75 x 3 x 3\n",
    "\n",
    "    # Flatten to Dense\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(flattened_img_size, 512),\n",
    "    nn.Dropout(.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, n_classes)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eHXRtWa3ZZP"
   },
   "source": [
    "\n",
    "\n",
    "These are our 2D convolutional layers. Small kernels will go over the input image and detect features that are important for classification. Earlier convolutions in the model will detect simple features such as lines. Later convolutions will detect more complex features. Let's look at our first Conv2D layer:\n",
    "```Python\n",
    "nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1)\n",
    "```\n",
    "25 refers to the number of filters that will be learned. Even though `kernel_size = 3`, PyTorch will assume we want 3 x 3 filters. Stride refer to the step size that the filter will take as it passes over the image. Padding refers to whether the output image that's created from the filter will match the size of the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiuMlsan3ZZQ"
   },
   "source": [
    "### 3.3.2 [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp72aAnK3ZZQ"
   },
   "source": [
    "Like normalizing our inputs, batch normalization scales the values in the hidden layers to improve training. [Read more about it in detail here](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/).\n",
    "\n",
    "There is a debate on best where to put the batch normalization layer. [This Stack Overflow post](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout) compiles many perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twarf_s63ZZQ"
   },
   "source": [
    "### 3.3.3 [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoNIzZZW3ZZQ"
   },
   "source": [
    "\n",
    "Max pooling takes an image and essentially shrinks it to a lower resolution. It does this to help the model be robust to translation (objects moving side to side), and also makes our model faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzHlBRja3ZZQ"
   },
   "source": [
    "### 3.3.4 [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJjrPvkm3ZZQ"
   },
   "source": [
    "\n",
    "Dropout is a technique for preventing overfitting. Dropout randomly selects a subset of neurons and turns them off, so that they do not participate in forward or backward propagation in that particular pass. This helps to make sure that the network is robust and redundant, and does not rely on any one area to come up with answers.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRYPkQPA3ZZQ"
   },
   "source": [
    "### 3.3.5 [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuMt-DpZ3ZZQ"
   },
   "source": [
    "Flatten takes the output of one layer which is multidimensional, and flattens it into a one-dimensional array. The output is called a feature vector and will be connected to the final classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSur4TGx3ZZQ"
   },
   "source": [
    "### 3.3.6 [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PATqMedY3ZZQ"
   },
   "source": [
    "We have seen dense linear layers before in our earlier models. Our first dense layer (512 units) takes the feature vector as input and learns which features will contribute to a particular classification. The second dense layer (24 units) is the final classification layer that outputs our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_opXKGWj3ZZQ"
   },
   "source": [
    "## 3.4 Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo6eRrp23ZZQ"
   },
   "source": [
    "This may feel like a lot of information, but don't worry. It's not critical that to understand everything right now in order to effectively train convolutional models. Most importantly we know that they can help with extracting useful information from images, and can be used in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5811,
     "status": "ok",
     "timestamp": 1732283246883,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "2IAS92gZwcP3",
    "outputId": "6f9a4011-2197-4397-d9af-31801d48d304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Sequential(\n",
       "    (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(25, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): Flatten(start_dim=1, end_dim=-1)\n",
       "    (14): Linear(in_features=675, out_features=512, bias=True)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): ReLU()\n",
       "    (17): Linear(in_features=512, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.compile()은 model을 최적화하여 GPU(device)에서 실행되도록 준비\n",
    "model = torch.compile(model.to(device))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9bOuIM1BMWQ"
   },
   "source": [
    "Since the problem we are trying to solve is still the same (classifying ASL images), we will continue to use the same `loss_function` and `accuracy` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BUIQ5COwsri"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SniWnvc5NSkA"
   },
   "outputs": [],
   "source": [
    "# y.view_as(pred)는 텐서 y를 pred와 동일한 형태로 변경.\n",
    "# y.view(pred.size())를 호출하는 것과 동일하지만, view_as() 함수가 더 편리한 경우가 많음\n",
    "\n",
    "def get_batch_accuracy(output, y, N):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBgbUNDH3ZZR"
   },
   "source": [
    "### 3.5 Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsS9zDKh3ZZR"
   },
   "source": [
    "Despite the very different model architecture, the training looks exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o-8EthRBMWR"
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEWasp-jBMWR"
   },
   "source": [
    "These are the same `train` and `validate` functions as before, but they have been mixed up. Can you correctly name each function and replace the `FIXME`s?\n",
    "\n",
    "One of them should have `model.train` and the other should have `model.eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9R0vJA8NQUW"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for x, y in train_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            # batch 32개의 loss 값을 누적하여 batch 평균 loss 값 계산\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr-X8QkVv9I7"
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Kzg57ArKgdJ"
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34930,
     "status": "ok",
     "timestamp": 1732283977303,
     "user": {
      "displayName": "JongHyun Kim",
      "userId": "04750869897114690179"
     },
     "user_tz": -540
    },
    "id": "qOYsrlmUwyyI",
    "outputId": "135ad726-683a-4712-cb0e-551f0a21f84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT inner /usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py line 38 \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] due to: \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1122 13:59:03.579000 3285 torch/_dynamo/convert_frame.py:1125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 2751.3776 Accuracy: 0.2080\n",
      "Valid - Loss: 717.5431 Accuracy: 0.0487\n",
      "Epoch: 1\n",
      "Train - Loss: 2753.2788 Accuracy: 0.2023\n",
      "Valid - Loss: 717.5198 Accuracy: 0.0496\n",
      "Epoch: 2\n",
      "Train - Loss: 2753.5545 Accuracy: 0.2078\n",
      "Valid - Loss: 717.5474 Accuracy: 0.0501\n",
      "Epoch: 3\n",
      "Train - Loss: 2754.0523 Accuracy: 0.2073\n",
      "Valid - Loss: 717.4795 Accuracy: 0.0501\n",
      "Epoch: 4\n",
      "Train - Loss: 2754.9783 Accuracy: 0.2004\n",
      "Valid - Loss: 717.5445 Accuracy: 0.0503\n",
      "Epoch: 5\n",
      "Train - Loss: 2754.1800 Accuracy: 0.2012\n",
      "Valid - Loss: 717.5343 Accuracy: 0.0499\n",
      "Epoch: 6\n",
      "Train - Loss: 2753.5429 Accuracy: 0.2016\n",
      "Valid - Loss: 717.5332 Accuracy: 0.0499\n",
      "Epoch: 7\n",
      "Train - Loss: 2753.2531 Accuracy: 0.2079\n",
      "Valid - Loss: 717.5212 Accuracy: 0.0512\n",
      "Epoch: 8\n",
      "Train - Loss: 2753.4257 Accuracy: 0.2059\n",
      "Valid - Loss: 717.5820 Accuracy: 0.0492\n",
      "Epoch: 9\n",
      "Train - Loss: 2753.1795 Accuracy: 0.1994\n",
      "Valid - Loss: 717.5402 Accuracy: 0.0495\n",
      "Epoch: 10\n",
      "Train - Loss: 2753.3618 Accuracy: 0.1959\n",
      "Valid - Loss: 717.5538 Accuracy: 0.0499\n",
      "Epoch: 11\n",
      "Train - Loss: 2750.7587 Accuracy: 0.2103\n",
      "Valid - Loss: 717.5838 Accuracy: 0.0496\n",
      "Epoch: 12\n",
      "Train - Loss: 2753.5469 Accuracy: 0.2030\n",
      "Valid - Loss: 717.5471 Accuracy: 0.0487\n",
      "Epoch: 13\n",
      "Train - Loss: 2752.3992 Accuracy: 0.2050\n",
      "Valid - Loss: 717.5400 Accuracy: 0.0512\n",
      "Epoch: 14\n",
      "Train - Loss: 2754.1846 Accuracy: 0.2023\n",
      "Valid - Loss: 717.5729 Accuracy: 0.0495\n",
      "Epoch: 15\n",
      "Train - Loss: 2754.3501 Accuracy: 0.2055\n",
      "Valid - Loss: 717.5474 Accuracy: 0.0496\n",
      "Epoch: 16\n",
      "Train - Loss: 2751.8425 Accuracy: 0.2071\n",
      "Valid - Loss: 717.5603 Accuracy: 0.0491\n",
      "Epoch: 17\n",
      "Train - Loss: 2752.9225 Accuracy: 0.2068\n",
      "Valid - Loss: 717.5364 Accuracy: 0.0498\n",
      "Epoch: 18\n",
      "Train - Loss: 2753.8174 Accuracy: 0.2055\n",
      "Valid - Loss: 717.5455 Accuracy: 0.0495\n",
      "Epoch: 19\n",
      "Train - Loss: 2754.1122 Accuracy: 0.1969\n",
      "Valid - Loss: 717.5412 Accuracy: 0.0495\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVytGlnl3ZZR"
   },
   "source": [
    "### 3.5.1 Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukd8Kk8l3ZZR"
   },
   "source": [
    "It looks like this model is significantly improved! The training accuracy is very high, and the validation accuracy has improved as well. This is a great result, as all we had to do was swap in a new model.\n",
    "\n",
    "You may have noticed the validation accuracy jumping around. This is an indication that our model is still not generalizing perfectly. Fortunately, there's more that we can do. Let's talk about it in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsOHIy5F3ZZR"
   },
   "source": [
    "## 3.6 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcIRdSur3ZZR"
   },
   "source": [
    "In this section, we utilized several new kinds of layers to implement a CNN, which performed better than the more simple model used in the last section. Hopefully the overall process of creating and training a model with prepared data is starting to become even more familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0wFCmbK3ZZS"
   },
   "source": [
    "### 3.6.1 Clear the Memory\n",
    "Before moving on, please execute the following cell to clear up the GPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ul7wgax3ZZS"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kMR2FOK3ZZS"
   },
   "source": [
    "### 3.6.2 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13FglbMX3ZZS"
   },
   "source": [
    "In the last several sections you have focused on the creation and training of models. In order to further improve performance, you will now turn your attention to *data augmentation*, a collection of techniques that will allow your models to train on more and better data than what you might have originally at your disposal."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZuABSmiRBMWI",
    "vYlZH7DNBMWJ"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
